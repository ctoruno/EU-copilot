"""
Project:        EU Copilot App
Module Name:    GPP Tools Page
Author:         Carlos Alberto ToruÃ±o Paniagua
Date:           October 19th, 2023
Description:    This module contains the code of the GPP Tools tab for the EU Copilot App
This version:   December 5th, 2023
"""

import pandas as pd
import streamlit as st
import tools.gpp_tools as gpp
import copy
from tools import sidemenu

# Page config
st.set_page_config(
    page_title = "GPP Tools",
    page_icon  = "ðŸ”¨"
)

# Reading CSS styles
with open("styles.css") as stl:
    st.markdown(f"<style>{stl.read()}</style>", 
                unsafe_allow_html=True)

# Sidebar menu
sidemenu.insert_smenu()

# Header and introduction
# st.markdown("<h1 style='text-align: center;'>GPP Tools</h1>", 
#             unsafe_allow_html=True)
st.markdown(
    """
    <p class='jtext'>
    Welcome to the <strong style="color:#003249">GPP toolkit</strong>. In this page you will find
    a set of tools to support your cleaning and harmonization routine writting. This toolkit has
    been developed according to the guidelines expressed in the <strong style="color:#003249">
    GPP Cleaning and Harmonization Protocol</strong>.
    
    The code generated by this toolkit is intended to serve as a support. Under no circumstance 
    use the Stata code lines generated by this toolkit without prior analysis of its impact. 
    </p>
    """,
    unsafe_allow_html = True
)

# Preview/Hide button
# if 'button' not in st.session_state:
#     st.session_state.button = False

# def click_button():
#     st.session_state.button = not st.session_state.button

# cb_preview = st.button("Click here to preview/hide the protocol",
#                         on_click = click_button)
# if st.session_state.button:
#     gpp.show_pdf("inputs/GPP-cleaning-and-validation-protocol.pdf")


# Download protocol buttton
with open("inputs/GPP-cleaning-and-validation-protocol.pdf", "rb") as pdf_file:
    PDFbyte = pdf_file.read()

st.download_button(label="Download Protocol", 
        data      = PDFbyte,
        file_name = "pandas-clean-id-column.pdf",
        mime      = 'application/octet-stream')

# Reading Codebook & DataMap
@st.cache_data
def load_datamap():
    datamap = pd.read_excel("inputs/EU2 GPP 2023 Full Datamap.xlsx", 
                            sheet_name = "Data Map")
    return datamap

@st.cache_data
def load_codebook():
    datamap = pd.read_excel("inputs/EU2 GPP 2023 Codebook.xlsx", 
                            sheet_name = "Codebook")
    return datamap

datamap  = load_datamap()
codebook = load_codebook() 

st.markdown("------")

# Creating a container for data loading
dataup_container = st.container()
with dataup_container:

    st.markdown("<h4>Data uploading</h4>",
                unsafe_allow_html = True)
    
    valabs = st.toggle(
        "Would you like to read the value labels?",
        value = False
    )
    
    # Uploader widget
    if valabs == True:
        st.write(
            """
            **IMPORTANT**: _We higly suggest uploading a DTA file with variable names fixed, 
            but NO labelling standardization performed_.
            """
            )
    uploaded_file = st.file_uploader("Upload a Stata DTA file", 
                                     type = ["dta"])
    
    upfile_duplicate = copy.copy(uploaded_file)
    
    # Read the uploaded file using Pandas
    if uploaded_file is not None:
        try:
            if valabs == False:
                data = pd.read_stata(uploaded_file,
                                     convert_categoricals = False,
                                     convert_dates        = False, 
                                     convert_missing      = False)

            # We need to read the data twice if we want to perform the value label checks                
            else:
                data = pd.read_stata(uploaded_file,
                                     convert_categoricals = False,
                                     convert_dates        = False, 
                                     convert_missing      = False)
                
                data_alpha = gpp.read_dta(upfile_duplicate)

            # Data preview expanders
            data_preview1 = st.expander("Click here to preview your data file")
            with data_preview1:
                st.write("Data from the uploaded file:")
                st.write(data)
            if valabs == True:
                data_preview2 = st.expander("Click here to preview your data file (with value labels)")
                with data_preview2:
                    st.write("Data from the uploaded file (with value labels):")
                    st.write(data_alpha)

        except pd.errors.ParserError as e:
            st.error("Error: Invalid DTA file. Please upload a valid Stata DTA file.")

    st.markdown("------")

st.markdown("<h4>Polling Company Information Lookup</h4>",
                unsafe_allow_html = True)
st.markdown("""
This section allows you to select a polling company from the dropdown menu to view its related information.
Upon selection, the app will display a list of countries where the selected polling company operates, 
alongside the main religion and the incumbent political party in those countries.
""")

eu_info_df = pd.read_csv("inputs/EU_general_information.csv", encoding = 'Windows-1252')  

company_list = eu_info_df['company'].unique()
selected_company = st.selectbox('Select a  Polling Company:', company_list)

if selected_company:
    filtered_df = eu_info_df[eu_info_df['company'] == selected_company]

    display_df = filtered_df[['country_name_ltn', 'main_religion', 'incumbent_party']].reset_index(drop=True)
    st.write('Results for selected Polling Company:')
    st.dataframe(display_df)

st.markdown("------")

# Creating a container for Checks
tools_container = st.container()
with tools_container:

    st.markdown("<h4>Checks results:</h4>",
                unsafe_allow_html = True)

    if uploaded_file is not None and valabs == False:

        # Widget to (de)activate case sensitivity
        scase = st.toggle(
            "Case sensitivity?",
            value = False,
            help = "When mapping missing variables, should the mapping omit mismatches based on lower/upper caps?"
        )

        # Applying the GPP tools
        dmap_missing, master_added = gpp.dtaNames(data, datamap, ignore_case = scase)

        # Displaying results within expanders
        exp2_1 = st.expander("Are all the variables listed in the data map present in the data set?")
        with exp2_1:
            st.markdown(
                """
                <p class='jtext'><b>
                The following elements were not found in the uploaded data file:
                </b></p>
                """,
                unsafe_allow_html = True
            )
            st.write(dmap_missing)
            st.markdown(
                """
                <p class='jtext'><b>
                Use the following Stata command line(s) to generate empty values:
                </b></p>
                """,
                unsafe_allow_html = True
            )
            for v in dmap_missing:
                st.code("g " + v + " = .",
                        language     = "stata", 
                        line_numbers = False)

        exp2_2 = st.expander("The following elements are present in the data file but they are not listed in the data map:")
        with exp2_2:
            st.markdown(
                """
                <p class='jtext'><b>
                The following elements are present in the data file but they are not listed in the data map:
                </b></p>
                """,
                unsafe_allow_html = True
            )
            st.write(master_added)
            st.markdown(
                """
                <p class='jtext'><b>
                Use the following Stata command line(s) to drop these variables:
                </b></p>
                """,
                unsafe_allow_html = True
            )
            st.code("drop " + " ".join(master_added),
                    language     = "stata", 
                    line_numbers = False)

        # Applying the additional check for range checks
        exp2_3 = st.expander("Does any variable surpass the expected value range?")
        with exp2_3:

            range_checks = gpp.dtaValues(data, datamap, dmap_missing)
            counts       = [sublist[0] for sublist in list(range_checks.values())]

            if sum(counts) == 0:
                st.markdown(
                """
                <p class='jtext'><b>
                No variable found with values outside the expected range
                </b></p>
                """,
                unsafe_allow_html = True
            )
            else:
                for column, result in range_checks.items():
                    if result[0] > 0 and len(result[1]) > 0:
                        st.code(f"At least one value in {column} is outside the expected range.")
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write("Variable presents the following frequencies:")
                            st.write(data[column].value_counts().sort_index())
                        with col2:
                            st.write("Available choices:")
                            st.write(pd.DataFrame(result[1]))
    
    # Separate container for value label checks
    elif uploaded_file is not None and valabs == True:

        st.markdown(
                """
                <p class='jtext'>
                <b>Inconsistencies in the value labels were found in the variables listed below.</b>
                
                This might be due to minor string additions such as typos, additional white spaces, etc.
                Please check the tables below to review the extent of the issue. 
                </p>
                """,
                unsafe_allow_html = True
            )
        
        # Filtering datamap for only those variables that we need to perform the check
        filtered_datamap = datamap[~(datamap["Label"].isin(["unchecked"]))]

        # Applying the GPP check for value labels
        results = [gpp.vals_checks(df_nums      = data, 
                                   df_labs      = data_alpha, 
                                   filtered_map = filtered_datamap, 
                                   target       = row["Variable"], 
                                   lab_class    = row["Label"])
                   for lab, row in filtered_datamap.iterrows()]
        
        # Filtering results for only those who DID NOT passed the test
        filtered_results = [x for x in results if x[1] == False]
        cols = [x[0] for x in results if x[1] == False]
        diff = [x[2] for x in results if x[1] == False]
        rule = [x[3] for x in results if x[1] == False]

        for i, x in enumerate(cols):
            with st.expander(f"Variable: {x}"):

                df1 = (data
                       .iloc[diff[i]]
                       .loc[:,x])
                df2 = (data_alpha
                       .iloc[diff[i]]
                       .loc[:,[x]]
                       .rename(columns = {x : f"{x}_alpha"}))

                valab_results = pd.concat([df1, df2], axis = 1)
                st.write("The value label rule is as follows:")
                st.write(rule[i])
                st.markdown("------")
                st.write("Below, you can see the variable encoded values along with their equivalent label. Only the rows were a mismatch was detected are shown:")
                st.write(valab_results)


    else:
        st.markdown("<p class='jtext'><b>Please upload a data file before continuing.</b></p>",
                    unsafe_allow_html = True
        )

                    